{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1396296a",
   "metadata": {},
   "source": [
    "# 🧺 Bagging Regression\n",
    "\n",
    "## 📌 What is Bagging?\n",
    "\n",
    "**Bagging (Bootstrap Aggregating)** is an ensemble technique that:\n",
    "- Reduces **variance**\n",
    "- Improves **stability and performance**\n",
    "- Works by combining predictions from **multiple regressors trained independently**\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Bagging for Regression\n",
    "\n",
    "### 🎯 Goal:\n",
    "Improve prediction accuracy by averaging outputs from multiple base regressors trained on **bootstrap samples**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Steps of Bagging Regression\n",
    "\n",
    "1. **Create bootstrap samples** from the training data  \n",
    "   (sampling **with replacement**)\n",
    "\n",
    "2. **Train a separate regressor** (e.g., decision tree) on each bootstrap sample\n",
    "\n",
    "3. **Aggregate predictions** using **mean**:\n",
    "   $$\n",
    "   \\hat{y}(x) = \\frac{1}{M} \\sum_{i=1}^{M} f_i(x)\n",
    "   $$\n",
    "   where $f_i(x)$ is the prediction from the $i^{th}$ model, $M$ is number of base regressors\n",
    "\n",
    "---\n",
    "\n",
    "## 🌳 Common Base Learner\n",
    "\n",
    "- **Decision Trees** are commonly used as base regressors because:\n",
    "  - They are **high-variance** models\n",
    "  - Bagging effectively reduces their variance\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Why Bagging Works for Regression\n",
    "\n",
    "- Reduces **overfitting** caused by variance in complex models\n",
    "- Helps generalize better on unseen data\n",
    "- Each model sees a **different subset**, adding diversity\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Advantages\n",
    "\n",
    "| Benefit               | Description                                      |\n",
    "|-----------------------|--------------------------------------------------|\n",
    "| Reduces Variance      | Combines unstable models (like trees) stably     |\n",
    "| Handles Overfitting   | Especially effective on complex, noisy datasets  |\n",
    "| Parallelizable        | Models are trained independently                 |\n",
    "| Easy to Implement     | Simple averaging strategy                        |\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ Limitations\n",
    "\n",
    "| Limitation            | Description                                      |\n",
    "|------------------------|--------------------------------------------------|\n",
    "| No Bias Reduction     | Doesn't fix underfitting (high bias)             |\n",
    "| Inefficient on Small Data | Bootstrap sampling may discard valuable data |\n",
    "| Less Interpretability | Multiple models make explanation harder          |\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 Scikit-Learn Example\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"OOB R^2 Score:\", model.oob_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14596fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
