{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b07d87",
   "metadata": {},
   "source": [
    "# ğŸ›ï¸ Sklearn Logistic Regression Hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® Core Parameters\n",
    "\n",
    "### ğŸ“Š **penalty** \n",
    "*Regularization technique to prevent overfitting by adding penalty terms to the loss function*\n",
    "\n",
    "| Value | Description | Effect | Use When |\n",
    "|-------|-------------|--------|----------|\n",
    "| `'l2'` | Ridge regularization | Shrinks all coefficients | **Default**, most cases |\n",
    "| `'l1'` | Lasso regularization | Feature selection (sparse) | Many irrelevant features |\n",
    "| `'elasticnet'` | L1 + L2 combination | Balanced regularization | Groups of correlated features |\n",
    "| `'none'` | No regularization | Risk of overfitting | Small datasets, simple models |\n",
    "\n",
    "### ğŸ¯ **C** (Regularization Strength)\n",
    "*Inverse of regularization strength - controls how much to penalize complex models*\n",
    "$C = \\frac{1}{\\lambda}$\n",
    "\n",
    "| Range | Effect | Model Behavior |\n",
    "|-------|--------|----------------|\n",
    "| **Small** (0.01-0.1) | Strong regularization | Simple, may underfit |\n",
    "| **Medium** (1.0) | Balanced | **Default**, good starting point |\n",
    "| **Large** (10-100) | Weak regularization | Complex, may overfit |\n",
    "\n",
    "### âš™ï¸ **solver**\n",
    "*Optimization algorithm used to find the best coefficients*\n",
    "\n",
    "| Solver | Best For | Supports |\n",
    "|--------|----------|----------|\n",
    "| `'lbfgs'` | Small datasets | L2, no penalty |\n",
    "| `'liblinear'` | **Binary** problems | L1, L2, no penalty |\n",
    "| `'newton-cg'` | Multinomial | L2, no penalty |\n",
    "| `'sag'` | Large datasets | L2, no penalty |\n",
    "| `'saga'` | **Large datasets** | All penalties |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Model Architecture\n",
    "\n",
    "### ğŸ”„ **multi_class**\n",
    "*Strategy for handling multi-class classification problems*\n",
    "\n",
    "| Value | Description | When to Use |\n",
    "|-------|-------------|-------------|\n",
    "| `'auto'` | Let sklearn decide | **Default** choice |\n",
    "| `'ovr'` | One-vs-Rest | Binary classifiers for each class |\n",
    "| `'multinomial'` | True multinomial | Single model for all classes |\n",
    "\n",
    "### âš–ï¸ **class_weight**\n",
    "*Weights associated with classes to handle imbalanced datasets*\n",
    "\n",
    "| Value | Effect | Use Case |\n",
    "|-------|--------|----------|\n",
    "| `None` | Equal weights | Balanced datasets |\n",
    "| `'balanced'` | Auto-balance | **Imbalanced** datasets |\n",
    "| `dict` | Custom weights | Specific class priorities |\n",
    "\n",
    "```python\n",
    "# Custom weights example\n",
    "class_weight = {0: 1, 1: 3}  # Class 1 is 3x more important\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ² Sampling & Initialization\n",
    "\n",
    "### ğŸ¯ **random_state**\n",
    "*Seed for random number generator to ensure reproducible results*\n",
    "```python\n",
    "random_state = 42  # Reproducible results\n",
    "```\n",
    "\n",
    "### ğŸ”€ **fit_intercept**\n",
    "*Whether to calculate the intercept (bias term) for the model*\n",
    "\n",
    "| Value | Effect | Use When |\n",
    "|-------|--------|----------|\n",
    "| `True` | Include bias term | **Default**, most cases |\n",
    "| `False` | No intercept | Pre-centered data |\n",
    "\n",
    "### ğŸ“Š **intercept_scaling**\n",
    "*Scaling factor for the synthetic intercept feature*\n",
    "- **Default**: `1.0`\n",
    "- **Effect**: Scales synthetic feature for intercept\n",
    "- **When to change**: Rarely needed\n",
    "\n",
    "---\n",
    "\n",
    "## â° Training Control\n",
    "\n",
    "### ğŸ”„ **max_iter**\n",
    "*Maximum number of iterations for the solver to converge*\n",
    "\n",
    "| Value | Effect | Use Case |\n",
    "|-------|--------|----------|\n",
    "| `100` | Default | Simple problems |\n",
    "| `1000+` | More iterations | Complex/large datasets |\n",
    "| `-1` | No limit | Until convergence |\n",
    "\n",
    "### ğŸ¯ **tol** (Tolerance)\n",
    "*Tolerance for stopping criteria - smaller values mean more precise results*\n",
    "\n",
    "| Value | Effect |\n",
    "|-------|--------|\n",
    "| `1e-4` | Default precision |\n",
    "| `1e-6` | Higher precision (slower) |\n",
    "| `1e-2` | Lower precision (faster) |\n",
    "\n",
    "### ğŸš€ **warm_start**\n",
    "*Whether to reuse the solution of the previous call to fit as initialization*\n",
    "\n",
    "| Value | Effect | Use Case |\n",
    "|-------|--------|----------|\n",
    "| `False` | Fresh start each fit | **Default** |\n",
    "| `True` | Continue from previous | Incremental learning |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›ï¸ Advanced Parameters\n",
    "\n",
    "### ğŸŒ **dual**\n",
    "*Whether to solve the dual or primal optimization problem*\n",
    "\n",
    "| Value | When to Use | Note |\n",
    "|-------|-------------|------|\n",
    "| `False` | n_samples > n_features | **Default** |\n",
    "| `True` | n_samples < n_features | Liblinear + L2 only |\n",
    "\n",
    "### ğŸ“ˆ **l1_ratio** (ElasticNet only)\n",
    "*Mixing parameter between L1 and L2 regularization (0 â‰¤ l1_ratio â‰¤ 1)*\n",
    "$\\text{Penalty} = \\alpha \\cdot [l1\\_ratio \\cdot ||\\mathbf{w}||_1 + (1-l1\\_ratio) \\cdot ||\\mathbf{w}||_2^2]$\n",
    "\n",
    "| Value | Effect |\n",
    "|-------|--------|\n",
    "| `0.0` | Pure L2 (Ridge) |\n",
    "| `0.5` | Equal L1 + L2 |\n",
    "| `1.0` | Pure L1 (Lasso) |\n",
    "\n",
    "### ğŸ”§ **solver_params**\n",
    "*Additional parameters passed to the solver*\n",
    "```python\n",
    "# For saga solver\n",
    "solver_params = {'saga': {'max_squared_sum': None}}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Complete Example\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ğŸ† Comprehensive hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'l1_ratio': [0.1, 0.5, 0.9]  # Only for elasticnet\n",
    "}\n",
    "\n",
    "# ğŸ” Grid search\n",
    "clf = LogisticRegression(random_state=42)\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick Start Configs\n",
    "\n",
    "### ğŸ“Š **Balanced Dataset**\n",
    "```python\n",
    "LogisticRegression(\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### âš–ï¸ **Imbalanced Dataset**\n",
    "```python\n",
    "LogisticRegression(\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    class_weight='balanced',\n",
    "    solver='liblinear',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸ¯ **Feature Selection**\n",
    "```python\n",
    "LogisticRegression(\n",
    "    C=0.1,\n",
    "    penalty='l1',\n",
    "    solver='liblinear',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸš€ **Large Dataset**\n",
    "```python\n",
    "LogisticRegression(\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    solver='saga',\n",
    "    max_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Performance Tips\n",
    "\n",
    "| Scenario | Recommended Settings |\n",
    "|----------|---------------------|\n",
    "| **Small dataset** | `solver='lbfgs'`, `C=1.0` |\n",
    "| **Large dataset** | `solver='saga'`, lower `tol` |\n",
    "| **Many features** | `penalty='l1'`, `solver='liblinear'` |\n",
    "| **Multiclass** | `solver='saga'`, `multi_class='multinomial'` |\n",
    "| **Speed priority** | `solver='liblinear'`, higher `tol` |\n",
    "| **Accuracy priority** | `solver='saga'`, lower `tol`, higher `max_iter` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b57cef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
