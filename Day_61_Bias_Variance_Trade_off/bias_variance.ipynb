{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e54e30b",
   "metadata": {},
   "source": [
    "# üéØ **Bias-Variance Tradeoff: A Comprehensive Guide**\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ **Table of Contents**\n",
    "\n",
    "1. [Introduction & Intuition](#introduction--intuition)\n",
    "2. [Mathematical Foundation](#mathematical-foundation)\n",
    "3. [Bias-Variance Decomposition](#bias-variance-decomposition)\n",
    "4. [Understanding Each Component](#understanding-each-component)\n",
    "5. [The Tradeoff Mechanism](#the-tradeoff-mechanism)\n",
    "6. [Model Complexity & Tradeoff](#model-complexity--tradeoff)\n",
    "7. [Visual Analysis & Examples](#visual-analysis--examples)\n",
    "8. [Practical Implications](#practical-implications)\n",
    "9. [Strategies to Manage Tradeoff](#strategies-to-manage-tradeoff)\n",
    "10. [Real-World Applications](#real-world-applications)\n",
    "11. [Advanced Concepts](#advanced-concepts)\n",
    "12. [Implementation & Simulation](#implementation--simulation)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Introduction & Intuition**\n",
    "\n",
    "### **What is the Bias-Variance Tradeoff?**\n",
    "\n",
    "The bias-variance tradeoff is a **fundamental concept** in machine learning that describes the relationship between three sources of error in predictive models:\n",
    "\n",
    "1. üéØ **Bias** - Error from oversimplified assumptions\n",
    "2. üìä **Variance** - Error from sensitivity to training data variations  \n",
    "3. üîä **Irreducible Error** - Inherent noise in the data\n",
    "\n",
    "### **The Core Dilemma**\n",
    "\n",
    "**You cannot minimize both bias and variance simultaneously!**\n",
    "\n",
    "- üìâ **Reducing bias** ‚Üí Often increases variance\n",
    "- üìà **Reducing variance** ‚Üí Often increases bias\n",
    "- üéØ **Goal:** Find the optimal balance\n",
    "\n",
    "### **Everyday Analogy: Archery Target**\n",
    "\n",
    "Think of prediction as **shooting arrows at a target**:\n",
    "\n",
    "üéØ **Low Bias, Low Variance:** Arrows clustered around bullseye ‚úÖ  \n",
    "üéØ **Low Bias, High Variance:** Arrows scattered around bullseye ‚ö†Ô∏è  \n",
    "üéØ **High Bias, Low Variance:** Arrows clustered away from bullseye ‚ö†Ô∏è  \n",
    "üéØ **High Bias, High Variance:** Arrows scattered away from bullseye ‚ùå\n",
    "\n",
    "### **Machine Learning Context**\n",
    "\n",
    "- **Target** = True function $f(x)$\n",
    "- **Arrows** = Model predictions $\\hat{f}(x)$\n",
    "- **Bullseye** = Perfect prediction\n",
    "- **Clustering** = Low variance\n",
    "- **Accuracy** = Low bias\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **Mathematical Foundation**\n",
    "\n",
    "### **Problem Setup**\n",
    "\n",
    "Given:\n",
    "- **True function:** $y = f(x) + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "- **Training dataset:** $\\mathcal{D} = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$\n",
    "- **Learned function:** $\\hat{f}_{\\mathcal{D}}(x)$ (depends on dataset $\\mathcal{D}$)\n",
    "\n",
    "### **Expected Test Error**\n",
    "\n",
    "For a new point $(x, y)$, the **expected squared error** is:\n",
    "\n",
    "$$\\boxed{E[(y - \\hat{f}(x))^2]}$$\n",
    "\n",
    "Where the expectation is taken over:\n",
    "- **Different training sets** $\\mathcal{D}$\n",
    "- **Noise** in the target $y$\n",
    "\n",
    "### **Key Insight**\n",
    "\n",
    "The error depends on:\n",
    "1. **Randomness in training data** ‚Üí Different $\\mathcal{D}$ give different $\\hat{f}$\n",
    "2. **Noise in target variable** ‚Üí $y$ has inherent randomness\n",
    "3. **Model's assumptions** ‚Üí How well model approximates truth\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **Bias-Variance Decomposition**\n",
    "\n",
    "### **The Fundamental Decomposition**\n",
    "\n",
    "The expected test error can be decomposed as:\n",
    "\n",
    "$$\\boxed{E[(y - \\hat{f}(x))^2] = \\text{Bias}^2[\\hat{f}(x)] + \\text{Var}[\\hat{f}(x)] + \\sigma^2}$$\n",
    "\n",
    "### **Detailed Mathematical Derivation**\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "**Step 1:** Expand the squared error\n",
    "$$E[(y - \\hat{f}(x))^2] = E[y^2] - 2E[y\\hat{f}(x)] + E[\\hat{f}(x)^2]$$\n",
    "\n",
    "**Step 2:** Use $y = f(x) + \\epsilon$ where $E[\\epsilon] = 0$\n",
    "$$E[y] = f(x), \\quad E[y^2] = f(x)^2 + \\sigma^2$$\n",
    "\n",
    "**Step 3:** Substitute and rearrange\n",
    "$$E[(y - \\hat{f}(x))^2] = f(x)^2 + \\sigma^2 - 2f(x)E[\\hat{f}(x)] + E[\\hat{f}(x)^2]$$\n",
    "\n",
    "**Step 4:** Add and subtract $E[\\hat{f}(x)]^2$\n",
    "$$= f(x)^2 - 2f(x)E[\\hat{f}(x)] + E[\\hat{f}(x)]^2 + E[\\hat{f}(x)^2] - E[\\hat{f}(x)]^2 + \\sigma^2$$\n",
    "\n",
    "**Step 5:** Recognize bias and variance terms\n",
    "$$\\boxed{= \\underbrace{(f(x) - E[\\hat{f}(x)])^2}_{\\text{Bias}^2} + \\underbrace{E[\\hat{f}(x)^2] - E[\\hat{f}(x)]^2}_{\\text{Variance}} + \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}}$$\n",
    "\n",
    "### **Component Definitions**\n",
    "\n",
    "#### **Bias**\n",
    "$$\\boxed{\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x)] - f(x)}$$\n",
    "\n",
    "**Measures:** How far the **average prediction** is from the **true value**\n",
    "\n",
    "#### **Variance**\n",
    "$$\\boxed{\\text{Var}[\\hat{f}(x)] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]}$$\n",
    "\n",
    "**Measures:** How much predictions **vary** across different training sets\n",
    "\n",
    "#### **Irreducible Error**\n",
    "$$\\boxed{\\sigma^2 = \\text{Var}[\\epsilon]}$$\n",
    "\n",
    "**Measures:** **Inherent noise** in the data that cannot be reduced\n",
    "\n",
    "---\n",
    "\n",
    "## üîç **Understanding Each Component**\n",
    "\n",
    "### **1. Bias - The Systematic Error**\n",
    "\n",
    "#### **Definition & Intuition**\n",
    "- **Bias** measures **systematic deviation** from truth\n",
    "- High bias = **Underfitting** = Model too simple\n",
    "- Low bias = Model captures underlying pattern well\n",
    "\n",
    "#### **Mathematical Properties**\n",
    "- $\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x)] - f(x)$\n",
    "- If unbiased: $E[\\hat{f}(x)] = f(x)$ ‚üπ $\\text{Bias} = 0$\n",
    "- Bias is **deterministic** for a given $x$\n",
    "\n",
    "#### **Sources of Bias**\n",
    "1. **Wrong model assumptions** (e.g., assuming linear when truth is nonlinear)\n",
    "2. **Insufficient model complexity**\n",
    "3. **Feature limitations**\n",
    "4. **Algorithmic constraints**\n",
    "\n",
    "#### **Examples of High Bias Models**\n",
    "- Linear regression for nonlinear data\n",
    "- Low-degree polynomials for complex curves\n",
    "- Naive Bayes with strong independence assumptions\n",
    "- Logistic regression for complex decision boundaries\n",
    "\n",
    "### **2. Variance - The Sensitivity Error**\n",
    "\n",
    "#### **Definition & Intuition**\n",
    "- **Variance** measures **sensitivity** to training data changes\n",
    "- High variance = **Overfitting** = Model too flexible\n",
    "- Low variance = Model stable across different datasets\n",
    "\n",
    "#### **Mathematical Properties**\n",
    "- $\\text{Var}[\\hat{f}(x)] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$\n",
    "- Equivalent form: $\\text{Var}[\\hat{f}(x)] = E[\\hat{f}(x)^2] - (E[\\hat{f}(x)])^2$\n",
    "- Variance is always **non-negative**\n",
    "\n",
    "#### **Sources of Variance**\n",
    "1. **Limited training data**\n",
    "2. **High model complexity**\n",
    "3. **Noisy features**\n",
    "4. **Random initialization effects**\n",
    "\n",
    "#### **Examples of High Variance Models**\n",
    "- High-degree polynomials\n",
    "- Deep neural networks (without regularization)\n",
    "- k-NN with small k\n",
    "- Decision trees with no pruning\n",
    "\n",
    "### **3. Irreducible Error - The Fundamental Limit**\n",
    "\n",
    "#### **Definition & Intuition**\n",
    "- **Irreducible error** is the **minimum possible error**\n",
    "- Represents **inherent randomness** in the system\n",
    "- Cannot be reduced by better models or more data\n",
    "\n",
    "#### **Mathematical Properties**\n",
    "- $\\sigma^2 = \\text{Var}[\\epsilon] = E[\\epsilon^2]$\n",
    "- Independent of model choice\n",
    "- Sets **lower bound** on achievable error\n",
    "\n",
    "#### **Sources of Irreducible Error**\n",
    "1. **Measurement noise**\n",
    "2. **Unobserved variables**\n",
    "3. **Inherent randomness** in the process\n",
    "4. **Model misspecification** at fundamental level\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è **The Tradeoff Mechanism**\n",
    "\n",
    "### **Why the Tradeoff Exists**\n",
    "\n",
    "#### **Simple Models (High Bias, Low Variance)**\n",
    "- **Make strong assumptions** ‚Üí High bias\n",
    "- **Consistent across datasets** ‚Üí Low variance\n",
    "- **Example:** Linear regression for nonlinear data\n",
    "\n",
    "#### **Complex Models (Low Bias, High Variance)**\n",
    "- **Few assumptions** ‚Üí Low bias  \n",
    "- **Highly sensitive** to data ‚Üí High variance\n",
    "- **Example:** High-degree polynomial regression\n",
    "\n",
    "### **Mathematical Relationship**\n",
    "\n",
    "As model complexity increases:\n",
    "\n",
    "$$\\boxed{\\begin{align}\n",
    "\\text{Bias} &\\searrow \\text{ (decreases)} \\\\\n",
    "\\text{Variance} &\\nearrow \\text{ (increases)} \\\\\n",
    "\\text{Total Error} &= \\text{Bias}^2 + \\text{Variance} + \\sigma^2\n",
    "\\end{align}}$$\n",
    "\n",
    "### **The Sweet Spot**\n",
    "\n",
    "**Optimal complexity** minimizes total error:\n",
    "\n",
    "$$\\boxed{\\text{Complexity}^* = \\arg\\min_{\\text{complexity}} [\\text{Bias}^2 + \\text{Variance} + \\sigma^2]}$$\n",
    "\n",
    "### **Visual Representation**\n",
    "\n",
    "```\n",
    "Error\n",
    "  ‚Üë\n",
    "  |     Total Error\n",
    "  |        ‚à©\n",
    "  |       / \\\n",
    "  |      /   \\\n",
    "  |     /     \\\n",
    "  |    /       \\_____ Bias¬≤\n",
    "  |___/              \n",
    "  |  /________________ œÉ¬≤\n",
    "  | /        \n",
    "  |/_______ Variance\n",
    "  |________________________‚Üí Model Complexity\n",
    "          ‚Üë\n",
    "     Optimal Point\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Model Complexity & Tradeoff**\n",
    "\n",
    "### **Linear Models**\n",
    "\n",
    "#### **Simple Linear Regression**\n",
    "$$\\hat{f}(x) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "- **Bias:** High if true relationship is nonlinear\n",
    "- **Variance:** Low (only 2 parameters to estimate)\n",
    "\n",
    "#### **Polynomial Regression**\n",
    "$$\\hat{f}(x) = \\sum_{i=0}^{d} \\beta_i x^i$$\n",
    "\n",
    "**As degree $d$ increases:**\n",
    "- **Bias:** $\\downarrow$ Can fit more complex curves\n",
    "- **Variance:** $\\uparrow$ More parameters, more sensitivity\n",
    "\n",
    "### **k-Nearest Neighbors**\n",
    "\n",
    "$$\\hat{f}(x) = \\frac{1}{k} \\sum_{i \\in N_k(x)} y_i$$\n",
    "\n",
    "**As $k$ decreases:**\n",
    "- **Bias:** $\\downarrow$ More local, flexible predictions  \n",
    "- **Variance:** $\\uparrow$ More sensitive to individual points\n",
    "\n",
    "**As $k$ increases:**\n",
    "- **Bias:** $\\uparrow$ More global, averaging effect\n",
    "- **Variance:** $\\downarrow$ Stable across datasets\n",
    "\n",
    "### **Decision Trees**\n",
    "\n",
    "**Tree depth controls complexity:**\n",
    "\n",
    "**Shallow trees (low complexity):**\n",
    "- **Bias:** $\\uparrow$ Cannot capture detailed patterns\n",
    "- **Variance:** $\\downarrow$ Consistent splits\n",
    "\n",
    "**Deep trees (high complexity):**\n",
    "- **Bias:** $\\downarrow$ Can fit training data perfectly\n",
    "- **Variance:** $\\uparrow$ Sensitive to small data changes\n",
    "\n",
    "### **Neural Networks**\n",
    "\n",
    "**Network capacity affects tradeoff:**\n",
    "\n",
    "**Few parameters:**\n",
    "- **Bias:** $\\uparrow$ Limited expressiveness\n",
    "- **Variance:** $\\downarrow$ Stable training\n",
    "\n",
    "**Many parameters:**\n",
    "- **Bias:** $\\downarrow$ Universal approximation\n",
    "- **Variance:** $\\uparrow$ Sensitive to initialization, data\n",
    "\n",
    "### **Ensemble Methods**\n",
    "\n",
    "**Key insight:** Ensembles can reduce variance without increasing bias!\n",
    "\n",
    "#### **Bagging (Bootstrap Aggregating)**\n",
    "$$\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}_b(x)$$\n",
    "\n",
    "- **Bias:** Same as base model\n",
    "- **Variance:** $\\downarrow$ Reduced by averaging\n",
    "\n",
    "#### **Boosting**\n",
    "Sequential fitting to reduce bias and variance strategically.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **Visual Analysis & Examples**\n",
    "\n",
    "### **Polynomial Regression Example**\n",
    "\n",
    "Consider true function: $f(x) = 1.5x - 0.3x^2 + 0.1x^3 + \\epsilon$\n",
    "\n",
    "#### **Degree 1 (Linear): High Bias, Low Variance**\n",
    "```python\n",
    "# Systematic underfit - curved data, straight line model\n",
    "# Predictions consistent across different datasets\n",
    "# High bias: Cannot capture curvature\n",
    "# Low variance: Same line regardless of training set\n",
    "```\n",
    "\n",
    "#### **Degree 3 (Optimal): Balanced**\n",
    "```python\n",
    "# Captures true cubic relationship well\n",
    "# Moderate sensitivity to training data\n",
    "# Low bias: Matches true function form\n",
    "# Moderate variance: Some variation across datasets\n",
    "```\n",
    "\n",
    "#### **Degree 10 (Overfit): Low Bias, High Variance**\n",
    "```python\n",
    "# Fits training data perfectly (memorization)\n",
    "# Wildly different across training sets\n",
    "# Low bias: Can represent any shape\n",
    "# High variance: Completely different curves per dataset\n",
    "```\n",
    "\n",
    "### **Learning Curves Analysis**\n",
    "\n",
    "#### **High Bias Scenario**\n",
    "```\n",
    "Error\n",
    "  ‚Üë\n",
    "  |  Training Error ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "  |                               \n",
    "  |  Validation Error ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "  |                             \n",
    "  |________________________‚Üí Training Set Size\n",
    "  \n",
    "  Both errors high and close together\n",
    "  More data doesn't help much\n",
    "```\n",
    "\n",
    "#### **High Variance Scenario**\n",
    "```\n",
    "Error\n",
    "  ‚Üë\n",
    "  |  Validation Error \\\n",
    "  |                    \\\n",
    "  |                     \\______\n",
    "  |  Training Error ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "  |________________________‚Üí Training Set Size\n",
    "  \n",
    "  Large gap between training and validation\n",
    "  More data helps significantly\n",
    "```\n",
    "\n",
    "#### **Well-Balanced Model**\n",
    "```\n",
    "Error\n",
    "  ‚Üë\n",
    "  |  Validation Error \\\n",
    "  |                    \\___\n",
    "  |  Training Error ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\\___\n",
    "  |________________________‚Üí Training Set Size\n",
    "  \n",
    "  Errors converge to reasonable level\n",
    "  Good generalization achieved\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Practical Implications**\n",
    "\n",
    "### **Model Selection Guidelines**\n",
    "\n",
    "#### **When to Choose Simple Models (Accept Higher Bias)**\n",
    "- ‚úÖ **Small datasets** - Avoid overfitting\n",
    "- ‚úÖ **High noise** - Complex models amplify noise\n",
    "- ‚úÖ **Interpretability needed** - Simple = explainable\n",
    "- ‚úÖ **Fast inference required** - Simple = fast\n",
    "- ‚úÖ **Limited computational resources**\n",
    "\n",
    "#### **When to Choose Complex Models (Accept Higher Variance)**\n",
    "- ‚úÖ **Large datasets** - Can estimate many parameters\n",
    "- ‚úÖ **Low noise** - Can afford to be sensitive\n",
    "- ‚úÖ **Complex underlying patterns** - Need flexibility\n",
    "- ‚úÖ **Performance critical** - Accuracy over interpretability\n",
    "\n",
    "### **Diagnostic Tools**\n",
    "\n",
    "#### **1. Training vs Validation Curves**\n",
    "```python\n",
    "def diagnose_bias_variance(train_errors, val_errors):\n",
    "    \"\"\"\n",
    "    Diagnose bias-variance issues from learning curves\n",
    "    \"\"\"\n",
    "    final_train_error = train_errors[-1]\n",
    "    final_val_error = val_errors[-1]\n",
    "    gap = final_val_error - final_train_error\n",
    "    \n",
    "    if final_val_error > threshold_high and gap < threshold_small:\n",
    "        return \"High Bias (Underfitting)\"\n",
    "    elif gap > threshold_large:\n",
    "        return \"High Variance (Overfitting)\"\n",
    "    else:\n",
    "        return \"Well Balanced\"\n",
    "```\n",
    "\n",
    "#### **2. Cross-Validation**\n",
    "- **High bias:** Both train and validation scores low\n",
    "- **High variance:** Large gap between train and validation scores\n",
    "- **Balanced:** Good scores with small gap\n",
    "\n",
    "#### **3. Bootstrap Analysis**\n",
    "Generate multiple models on bootstrap samples:\n",
    "- **High variance:** Predictions vary significantly\n",
    "- **Low variance:** Predictions consistent\n",
    "\n",
    "### **Data Size Considerations**\n",
    "\n",
    "#### **Small Data Regime**\n",
    "- **Variance dominates** - Limited samples to estimate parameters\n",
    "- **Prefer simpler models** - Linear, low-degree polynomials\n",
    "- **Regularization crucial** - Prevent overfitting\n",
    "\n",
    "#### **Large Data Regime**  \n",
    "- **Bias becomes more important** - Can afford complex models\n",
    "- **Complex models viable** - Neural networks, high-degree polynomials  \n",
    "- **Computational efficiency matters** - Training cost scales\n",
    "\n",
    "### **Feature Engineering Impact**\n",
    "\n",
    "#### **Too Few Features**\n",
    "- **High bias** - Cannot represent complex relationships\n",
    "- **Low variance** - Few parameters to estimate\n",
    "\n",
    "#### **Too Many Features**\n",
    "- **Low bias** - Rich representation possible\n",
    "- **High variance** - Many parameters, curse of dimensionality\n",
    "\n",
    "#### **Optimal Feature Set**\n",
    "- **Balanced representation** - Captures important patterns\n",
    "- **Dimensionality appropriate** for data size\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **Strategies to Manage Tradeoff**\n",
    "\n",
    "### **1. Regularization Techniques**\n",
    "\n",
    "#### **Ridge Regression (L2)**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|^2$$\n",
    "\n",
    "**Effect:**\n",
    "- **Bias:** $\\uparrow$ Shrinks coefficients toward zero\n",
    "- **Variance:** $\\downarrow$ Reduces parameter sensitivity\n",
    "- **Net effect:** Often improves generalization\n",
    "\n",
    "#### **Lasso Regression (L1)**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1$$\n",
    "\n",
    "**Effect:**\n",
    "- **Bias:** $\\uparrow$ Sets some coefficients to exactly zero\n",
    "- **Variance:** $\\downarrow$ Automatic feature selection\n",
    "- **Net effect:** Sparse, interpretable models\n",
    "\n",
    "#### **Elastic Net**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2 + \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\lambda_2 \\|\\boldsymbol{\\beta}\\|^2$$\n",
    "\n",
    "**Effect:** Combines L1 and L2 benefits\n",
    "\n",
    "### **2. Cross-Validation for Model Selection**\n",
    "\n",
    "#### **k-Fold Cross-Validation**\n",
    "```python\n",
    "def select_optimal_complexity(X, y, complexities, k=5):\n",
    "    \"\"\"\n",
    "    Select model complexity that minimizes CV error\n",
    "    \"\"\"\n",
    "    cv_errors = []\n",
    "    \n",
    "    for complexity in complexities:\n",
    "        fold_errors = []\n",
    "        \n",
    "        for fold in k_fold_split(X, y, k):\n",
    "            X_train, X_val, y_train, y_val = fold\n",
    "            \n",
    "            model = train_model(X_train, y_train, complexity)\n",
    "            error = evaluate_model(model, X_val, y_val)\n",
    "            fold_errors.append(error)\n",
    "        \n",
    "        cv_errors.append(np.mean(fold_errors))\n",
    "    \n",
    "    optimal_complexity = complexities[np.argmin(cv_errors)]\n",
    "    return optimal_complexity\n",
    "```\n",
    "\n",
    "### **3. Ensemble Methods**\n",
    "\n",
    "#### **Bagging (Reduce Variance)**\n",
    "$$\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}_b(x)$$\n",
    "\n",
    "**Mechanism:**\n",
    "- Train multiple models on bootstrap samples\n",
    "- Average predictions reduces variance\n",
    "- **Bias unchanged, Variance reduced**\n",
    "\n",
    "#### **Random Forest Example**\n",
    "```python\n",
    "class BiasVarianceRandomForest:\n",
    "    def __init__(self, n_trees=100, max_depth=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            # Bootstrap sample\n",
    "            bootstrap_indices = np.random.choice(\n",
    "                len(X), size=len(X), replace=True\n",
    "            )\n",
    "            X_boot = X[bootstrap_indices]\n",
    "            y_boot = y[bootstrap_indices]\n",
    "            \n",
    "            # Train tree\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Average predictions (reduces variance)\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(predictions, axis=0)\n",
    "```\n",
    "\n",
    "#### **Boosting (Reduce Bias)**\n",
    "Sequential learning to focus on mistakes:\n",
    "\n",
    "**AdaBoost Example:**\n",
    "```python\n",
    "# Each new model focuses on previously misclassified examples\n",
    "# Gradually reduces bias by learning complex patterns\n",
    "# Can increase variance if not carefully controlled\n",
    "```\n",
    "\n",
    "### **4. Early Stopping**\n",
    "\n",
    "For iterative algorithms (neural networks, gradient boosting):\n",
    "\n",
    "```python\n",
    "def train_with_early_stopping(model, X_train, y_train, X_val, y_val):\n",
    "    best_val_error = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 10\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train_one_epoch(X_train, y_train)\n",
    "        \n",
    "        val_error = model.evaluate(X_val, y_val)\n",
    "        \n",
    "        if val_error < best_val_error:\n",
    "            best_val_error = val_error\n",
    "            patience_counter = 0\n",
    "            model.save_checkpoint()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            model.load_checkpoint()  # Revert to best model\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "### **5. Data Augmentation**\n",
    "\n",
    "**Increase effective training set size:**\n",
    "- **Computer Vision:** Rotations, crops, flips\n",
    "- **NLP:** Paraphrasing, back-translation\n",
    "- **Time Series:** Jittering, time-warping\n",
    "\n",
    "**Effect:** More data ‚Üí Can handle more complex models ‚Üí Better bias-variance balance\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Real-World Applications**\n",
    "\n",
    "### **Computer Vision**\n",
    "\n",
    "#### **Image Classification**\n",
    "- **Simple models:** Linear classifiers on raw pixels\n",
    "  - **High bias:** Cannot capture complex visual patterns\n",
    "  - **Low variance:** Stable across datasets\n",
    "  \n",
    "- **Complex models:** Deep CNNs\n",
    "  - **Low bias:** Can learn hierarchical features\n",
    "  - **High variance:** Sensitive to training data\n",
    "  \n",
    "- **Solution:** Transfer learning, data augmentation, regularization\n",
    "\n",
    "#### **Object Detection**\n",
    "- **Tradeoff:** Speed vs accuracy\n",
    "- **Fast models:** Higher bias, lower computational cost\n",
    "- **Accurate models:** Lower bias, higher computational cost\n",
    "\n",
    "### **Natural Language Processing**\n",
    "\n",
    "#### **Text Classification**\n",
    "- **Bag-of-words:** High bias (ignores word order)\n",
    "- **Transformers:** Low bias (captures context)\n",
    "- **Management:** Pre-training, fine-tuning, regularization\n",
    "\n",
    "#### **Machine Translation**\n",
    "- **Rule-based:** High bias (fixed rules)\n",
    "- **Neural:** Low bias (learned patterns)\n",
    "- **Ensemble:** Combine multiple approaches\n",
    "\n",
    "### **Time Series Forecasting**\n",
    "\n",
    "#### **Simple Models**\n",
    "- **ARIMA:** Assumes linear relationships\n",
    "- **High bias** for complex patterns\n",
    "- **Low variance:** Stable parameters\n",
    "\n",
    "#### **Complex Models**  \n",
    "- **LSTM/GRU:** Can capture nonlinear patterns\n",
    "- **Low bias** for complex sequences\n",
    "- **High variance:** Many parameters\n",
    "\n",
    "#### **Practical Approach**\n",
    "```python\n",
    "def ensemble_forecast(models, X):\n",
    "    \"\"\"\n",
    "    Combine simple and complex models\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    weights = []\n",
    "    \n",
    "    for model, weight in models:\n",
    "        pred = model.predict(X)\n",
    "        predictions.append(pred)\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Weighted average reduces variance while maintaining low bias\n",
    "    ensemble_pred = np.average(predictions, weights=weights, axis=0)\n",
    "    return ensemble_pred\n",
    "```\n",
    "\n",
    "### **Recommendation Systems**\n",
    "\n",
    "#### **Collaborative Filtering**\n",
    "- **Memory-based:** High bias (simple similarity)\n",
    "- **Matrix factorization:** Balanced bias-variance\n",
    "- **Deep learning:** Low bias, potential high variance\n",
    "\n",
    "#### **Hybrid Approaches**\n",
    "Combine multiple techniques to balance bias-variance effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Advanced Concepts**\n",
    "\n",
    "### **Bias-Variance in Different Learning Paradigms**\n",
    "\n",
    "#### **Supervised Learning**\n",
    "- **Classification:** Bias-variance tradeoff in decision boundaries\n",
    "- **Regression:** Bias-variance in function approximation\n",
    "\n",
    "#### **Unsupervised Learning**\n",
    "- **Clustering:** Bias in cluster assumptions vs variance in cluster assignments\n",
    "- **Dimensionality Reduction:** Bias in linear assumptions vs variance in embeddings\n",
    "\n",
    "#### **Reinforcement Learning**\n",
    "- **Value Functions:** Bias in Bellman approximation vs variance in sampling\n",
    "- **Policy Learning:** Bias in policy parametrization vs variance in exploration\n",
    "\n",
    "### **Bayesian Perspective**\n",
    "\n",
    "#### **Prior Distributions**\n",
    "Strong priors ‚Üí **Higher bias**, **Lower variance**\n",
    "Weak priors ‚Üí **Lower bias**, **Higher variance**\n",
    "\n",
    "#### **Posterior Inference**\n",
    "$$p(\\theta | \\mathcal{D}) \\propto p(\\mathcal{D} | \\theta) p(\\theta)$$\n",
    "\n",
    "**Bias-Variance decomposition in Bayesian setting:**\n",
    "$$\\boxed{E[(y - \\hat{y})^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Noise}}$$\n",
    "\n",
    "Where expectations are over posterior distributions.\n",
    "\n",
    "### **Information-Theoretic View**\n",
    "\n",
    "#### **Model Capacity**\n",
    "- **VC Dimension:** Measures model complexity\n",
    "- **Rademacher Complexity:** Generalization bounds\n",
    "\n",
    "#### **Minimum Description Length (MDL)**\n",
    "Balance between:\n",
    "- **Model complexity** (longer description)\n",
    "- **Data fit** (shorter description of residuals)\n",
    "\n",
    "### **Online Learning**\n",
    "\n",
    "#### **Regret Bounds**\n",
    "$$\\text{Regret}_T = \\sum_{t=1}^{T} \\ell(f_t, (x_t, y_t)) - \\min_f \\sum_{t=1}^{T} \\ell(f, (x_t, y_t))$$\n",
    "\n",
    "**Bias-variance emerges** in regret analysis through:\n",
    "- **Approximation error** (bias)\n",
    "- **Estimation error** (variance)\n",
    "\n",
    "### **Multi-Task Learning**\n",
    "\n",
    "#### **Shared Representations**\n",
    "- **High sharing:** Lower variance, potential higher bias\n",
    "- **Low sharing:** Higher variance, potentially lower bias\n",
    "\n",
    "#### **Meta-Learning**\n",
    "Learn bias-variance tradeoff across tasks:\n",
    "```python\n",
    "def meta_learn_complexity(tasks, complexities):\n",
    "    \"\"\"\n",
    "    Learn optimal complexity for new tasks\n",
    "    \"\"\"\n",
    "    task_performances = {}\n",
    "    \n",
    "    for task in tasks:\n",
    "        for complexity in complexities:\n",
    "            performance = evaluate_on_task(task, complexity)\n",
    "            task_performances[(task, complexity)] = performance\n",
    "    \n",
    "    # Learn mapping from task features to optimal complexity\n",
    "    optimal_complexity_predictor = train_predictor(task_performances)\n",
    "    return optimal_complexity_predictor\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üíª **Implementation & Simulation**\n",
    "\n",
    "### **Bias-Variance Decomposition Simulation**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def bias_variance_decomposition(model_class, X_test, y_test_true, \n",
    "                               n_simulations=100, **model_params):\n",
    "    \"\"\"\n",
    "    Empirical bias-variance decomposition\n",
    "    \"\"\"\n",
    "    n_test = len(X_test)\n",
    "    predictions = np.zeros((n_simulations, n_test))\n",
    "    \n",
    "    # Generate multiple training sets and models\n",
    "    for i in range(n_simulations):\n",
    "        # Generate noisy training data\n",
    "        X_train, y_train = generate_training_data()\n",
    "        \n",
    "        # Train model\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test set\n",
    "        predictions[i] = model.predict(X_test)\n",
    "    \n",
    "    # Compute bias and variance\n",
    "    mean_predictions = np.mean(predictions, axis=0)\n",
    "    bias_squared = np.mean((mean_predictions - y_test_true) ** 2)\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    return bias_squared, variance\n",
    "\n",
    "def generate_training_data(n_samples=100, noise_std=0.3):\n",
    "    \"\"\"Generate training data from true function\"\"\"\n",
    "    X = np.random.uniform(-1, 1, n_samples).reshape(-1, 1)\n",
    "    y_true = true_function(X.ravel())\n",
    "    y_noisy = y_true + np.random.normal(0, noise_std, n_samples)\n",
    "    return X, y_noisy\n",
    "\n",
    "def true_function(x):\n",
    "    \"\"\"True underlying function\"\"\"\n",
    "    return 1.5 * x - 0.5 * x**2 + 0.3 * np.sin(15 * x)\n",
    "\n",
    "# Simulation\n",
    "X_test = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "y_test_true = true_function(X_test.ravel())\n",
    "\n",
    "models_to_test = {\n",
    "    'Linear': (LinearRegression, {}),\n",
    "    'Poly_2': (Pipeline, [\n",
    "        ('poly', PolynomialFeatures(2)),\n",
    "        ('linear', LinearRegression())\n",
    "    ]),\n",
    "    'Poly_5': (Pipeline, [\n",
    "        ('poly', PolynomialFeatures(5)),  \n",
    "        ('linear', LinearRegression())\n",
    "    ]),\n",
    "    'Poly_15': (Pipeline, [\n",
    "        ('poly', PolynomialFeatures(15)),\n",
    "        ('linear', LinearRegression())\n",
    "    ]),\n",
    "    'RandomForest': (RandomForestRegressor, {'n_estimators': 100})\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (model_class, params) in models_to_test.items():\n",
    "    bias_sq, variance = bias_variance_decomposition(\n",
    "        model_class, X_test, y_test_true, **params\n",
    "    )\n",
    "    total_error = bias_sq + variance + 0.3**2  # Add noise variance\n",
    "    \n",
    "    results[name] = {\n",
    "        'bias_squared': bias_sq,\n",
    "        'variance': variance,\n",
    "        'total_error': total_error\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Bias¬≤: {bias_sq:.4f}\")\n",
    "    print(f\"  Variance: {variance:.4f}\")\n",
    "    print(f\"  Total Error: {total_error:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf0adbb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
