{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e0dddd",
   "metadata": {},
   "source": [
    "### Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2eb953",
   "metadata": {},
   "source": [
    "#### 1) MAE - Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d190ee",
   "metadata": {},
   "source": [
    "# üìå Mean Absolute Error (MAE) ‚Äî Regression Metric\n",
    "\n",
    "## üß† Definition\n",
    "\n",
    "**Mean Absolute Error (MAE)** is a regression metric that measures the average magnitude of the errors between predicted and actual values, without considering their direction (i.e., positive or negative).\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the total number of data points\n",
    "- $y_i$ is the true value\n",
    "- $\\hat{y}_i$ is the predicted value\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Intuition\n",
    "\n",
    "- MAE gives equal weight to all errors.\n",
    "- It is a **linear score**, meaning each individual difference contributes proportionally to the total error.\n",
    "- Unlike Mean Squared Error (MSE), **MAE is more robust to outliers**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Example\n",
    "\n",
    "Suppose we have the following true and predicted values:\n",
    "\n",
    "| $y$ (True) | $\\hat{y}$ (Predicted) |\n",
    "|-----------|------------------------|\n",
    "| 3         | 2.5                    |\n",
    "| -0.5      | 0.0                    |\n",
    "| 2         | 2                      |\n",
    "| 7         | 8                      |\n",
    "\n",
    "Then the MAE is:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{4} \\left( |3 - 2.5| + |-0.5 - 0.0| + |2 - 2| + |7 - 8| \\right) = \\frac{1}{4}(0.5 + 0.5 + 0 + 1) = 0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Pros\n",
    "\n",
    "- Easy to understand and interpret.\n",
    "- Robust to outliers compared to MSE.\n",
    "- Same unit as target variable.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Cons\n",
    "\n",
    "- Gradient is not smooth at zero (less useful for optimization in some models).\n",
    "- Does not penalize large errors more than small ones (unlike MSE).\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Code (Python Example)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f83133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4b190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dfb6b6b",
   "metadata": {},
   "source": [
    "#### 2) MSE - Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c5c1b",
   "metadata": {},
   "source": [
    "# üìå Mean Squared Error (MSE) ‚Äî Regression Metric\n",
    "\n",
    "## üß† Definition\n",
    "\n",
    "**Mean Squared Error (MSE)** is a commonly used regression metric that measures the average of the squares of the errors between predicted and actual values.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of data points,\n",
    "- $y_i$ is the actual (true) value,\n",
    "- $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Intuition\n",
    "\n",
    "- Squaring the errors ensures they are positive and penalizes larger errors more heavily than smaller ones.\n",
    "- MSE is **sensitive to outliers**, since large errors are squared and thus magnified.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Example\n",
    "\n",
    "Given:\n",
    "\n",
    "| $y$ (True) | $\\hat{y}$ (Predicted) |\n",
    "|-----------|------------------------|\n",
    "| 3         | 2.5                    |\n",
    "| -0.5      | 0.0                    |\n",
    "| 2         | 2                      |\n",
    "| 7         | 8                      |\n",
    "\n",
    "Calculate:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MSE} &= \\frac{1}{4} \\left( (3 - 2.5)^2 + (-0.5 - 0)^2 + (2 - 2)^2 + (7 - 8)^2 \\right) \\\\\n",
    "&= \\frac{1}{4} \\left( 0.25 + 0.25 + 0 + 1 \\right) = \\frac{1.5}{4} = 0.375\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Pros\n",
    "\n",
    "- Penalizes large errors more, which is useful when large mistakes are more costly.\n",
    "- Smooth gradient ‚Äî useful in many optimization algorithms (e.g., gradient descent).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Cons\n",
    "\n",
    "- Not robust to outliers ‚Äî large errors dominate the metric.\n",
    "- The result is in **squared units** of the target variable, which can be less interpretable.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Code (Python Example)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"MSE: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8d6c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed16af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a670c1b",
   "metadata": {},
   "source": [
    "#### 3) RMSE - Root Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab434c30",
   "metadata": {},
   "source": [
    "# üìå Root Mean Squared Error (RMSE) ‚Äî Regression Metric\n",
    "\n",
    "## üß† Definition\n",
    "\n",
    "**Root Mean Squared Error (RMSE)** is the square root of the Mean Squared Error (MSE). It measures the average magnitude of the error in the same units as the target variable.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2 }\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of observations,\n",
    "- $y_i$ is the actual value,\n",
    "- $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Intuition\n",
    "\n",
    "- RMSE provides a **scale-sensitive** error metric ‚Äî large errors are penalized more than small ones (due to squaring).\n",
    "- Since the square root is applied, RMSE has the **same units as the target**, making it easier to interpret than MSE.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Example\n",
    "\n",
    "Let‚Äôs use the same values as in the MSE example:\n",
    "\n",
    "| $y$ (True) | $\\hat{y}$ (Predicted) |\n",
    "|-----------|------------------------|\n",
    "| 3         | 2.5                    |\n",
    "| -0.5      | 0.0                    |\n",
    "| 2         | 2                      |\n",
    "| 7         | 8                      |\n",
    "\n",
    "We already calculated the MSE:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = 0.375\n",
    "$$\n",
    "\n",
    "So RMSE is:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{0.375} \\approx 0.612\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Pros\n",
    "\n",
    "- Same unit as the target variable ‚Äî easy to interpret.\n",
    "- Useful when large errors are especially undesirable (e.g., forecasting problems).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Cons\n",
    "\n",
    "- Like MSE, it is **sensitive to outliers**.\n",
    "- Does not give information about direction of error (i.e., overestimation vs. underestimation).\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Code (Python Example)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3721f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db31557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6123724356957945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e2592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e1b0dc2",
   "metadata": {},
   "source": [
    "#### 4) R2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f919c8",
   "metadata": {},
   "source": [
    "# üìå $R^2$ Score ‚Äî Coefficient of Determination\n",
    "\n",
    "## üß† Definition\n",
    "\n",
    "The **$R^2$ Score** measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{ \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 }{ \\sum_{i=1}^n (y_i - \\bar{y})^2 }\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the actual value,\n",
    "- $\\hat{y}_i$ is the predicted value,\n",
    "- $\\bar{y}$ is the mean of the actual values,\n",
    "- $n$ is the number of samples.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Intuition\n",
    "\n",
    "- The numerator is the **Residual Sum of Squares (RSS)**.\n",
    "- The denominator is the **Total Sum of Squares (TSS)**.\n",
    "- $R^2$ tells us **how well the model explains the variance** in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Interpretation\n",
    "\n",
    "- $R^2 = 1$: Perfect prediction.\n",
    "- $R^2 = 0$: Model is no better than predicting the mean $\\bar{y}$.\n",
    "- $R^2 < 0$: Model is worse than simply using the mean.\n",
    "\n",
    "> A higher $R^2$ indicates a better fit, but **does not guarantee** a good model (especially if assumptions are violated).\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Example\n",
    "\n",
    "Let:\n",
    "\n",
    "- $y = [3, -0.5, 2, 7]$\n",
    "- $\\hat{y} = [2.5, 0.0, 2, 8]$\n",
    "\n",
    "Step 1: Compute mean of true values:\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{3 + (-0.5) + 2 + 7}{4} = 2.875\n",
    "$$\n",
    "\n",
    "Step 2: Compute TSS and RSS:\n",
    "\n",
    "$$\n",
    "\\text{TSS} = \\sum (y_i - \\bar{y})^2 = (3 - 2.875)^2 + (-0.5 - 2.875)^2 + (2 - 2.875)^2 + (7 - 2.875)^2 = 29.1875\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{RSS} = \\sum (y_i - \\hat{y}_i)^2 = 0.25 + 0.25 + 0 + 1 = 1.5\n",
    "$$\n",
    "\n",
    "Step 3: Compute $R^2$:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{1.5}{29.1875} \\approx 0.9486\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Pros\n",
    "\n",
    "- Indicates **how well the model fits** the data.\n",
    "- Easy to interpret ‚Äî closer to 1 means better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Cons\n",
    "\n",
    "- **Not reliable** for non-linear models.\n",
    "- **Can be misleading** when used alone.\n",
    "- **Does not indicate** whether predictions are biased or correct.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Code (Python Example)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83449de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.9486081370449679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac8454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ca1039f",
   "metadata": {},
   "source": [
    "#### 5) Adjusted R2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79869763",
   "metadata": {},
   "source": [
    "# üìå Adjusted $R^2$ Score ‚Äî Corrected Coefficient of Determination\n",
    "\n",
    "## üß† What is $R^2$?\n",
    "\n",
    "The **$R^2$ score** (Coefficient of Determination) measures how well the regression model explains the variance in the target variable. It is defined as:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{ \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 }{ \\sum_{i=1}^n (y_i - \\bar{y})^2 }\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ = actual value,\n",
    "- $\\hat{y}_i$ = predicted value,\n",
    "- $\\bar{y}$ = mean of actual values,\n",
    "- The numerator is the **Residual Sum of Squares (RSS)**,\n",
    "- The denominator is the **Total Sum of Squares (TSS)**.\n",
    "\n",
    "> An $R^2$ of 1 means perfect predictions. An $R^2$ of 0 means predictions are as good as the mean. Negative values imply worse than mean prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What is Adjusted $R^2$?\n",
    "\n",
    "While $R^2$ increases with more features, **Adjusted $R^2$ penalizes unnecessary features** to discourage overfitting. It adjusts $R^2$ based on the number of predictors used.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "R^2_{\\text{adj}} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $R^2$ = regular coefficient of determination,\n",
    "- $n$ = number of samples,\n",
    "- $p$ = number of features (independent variables).\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Intuition\n",
    "\n",
    "- If adding a feature doesn't improve $R^2$ much, Adjusted $R^2$ **goes down**.\n",
    "- Encourages **parsimonious models** (simpler with fewer variables).\n",
    "- Helpful for comparing models with different numbers of features.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Example\n",
    "\n",
    "Assume:\n",
    "- $R^2 = 0.90$\n",
    "- $n = 100$ samples\n",
    "- $p = 5$ features\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "R^2_{\\text{adj}} = 1 - \\left( \\frac{(1 - 0.90)(100 - 1)}{100 - 5 - 1} \\right)\n",
    "= 1 - \\left( \\frac{0.10 \\cdot 99}{94} \\right)\n",
    "= 1 - \\left( \\frac{9.9}{94} \\right)\n",
    "\\approx 1 - 0.1053 = 0.8947\n",
    "$$\n",
    "\n",
    "So, adjusted $R^2 \\approx 0.895$.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Pros\n",
    "\n",
    "- **Prevents overfitting** by penalizing extra features.\n",
    "- Useful when comparing models with **different numbers of predictors**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå Cons\n",
    "\n",
    "- Only suitable for **linear regression**.\n",
    "- Still doesn't show how large or biased the errors are.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Code (Python Example)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def adjusted_r2(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n",
    "\n",
    "# Example\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "n = len(y_true)\n",
    "p = 2  # Number of features used\n",
    "\n",
    "adj_r2 = adjusted_r2(y_true, y_pred, n, p)\n",
    "print(f\"Adjusted R^2: {adj_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916ceec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
